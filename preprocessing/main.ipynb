{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF fitz openai tiktoken numpy pandas umap-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/badlogic/workspaces/wahlomat/.venv/lib/python3.11/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdftools import *\n",
    "from llm import clear_history, complete, print_history, client, num_tokens\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "OUTPUT_DIR = \"../html/data/\"\n",
    "CLEAN = False\n",
    "START_STOP = {\n",
    "    \"fpö\": [4, 15],\n",
    "    \"grüne\": [4, 106],\n",
    "    \"kpö\": [4, 27],\n",
    "    \"neos\": [4, 39],\n",
    "    \"övp\": [3, 56],\n",
    "    \"spö\": [4, 26]\n",
    "}\n",
    "\n",
    "def summarize_file(party, filename, outputfile):    \n",
    "    file_content = read_file(filename)\n",
    "    pages = file_content.split('========== PAGE ')\n",
    "    summaries = []\n",
    "    start_stop = START_STOP[party]\n",
    "\n",
    "    for page in pages[1:]:        \n",
    "        page_number, page_text = page.split('\\n', 1)   \n",
    "        print(f\"Page {page_number}/{len(pages) - 1}\")\n",
    "        if int(page_number) < start_stop[0] or int(page_number) > start_stop[1] or not page_text.strip():        \n",
    "            print(\"Skipping\")\n",
    "            summary = ''\n",
    "        else:\n",
    "            clear_history()              \n",
    "            summary = complete(f\"\"\"\n",
    "Dieser Text ist Teil eines Wahlprogrammers. Extrahiere die Schlüsselpunkte des Textes, der durch 3 Backticks delimitiert ist, mit einem speziellen Fokus auf Forderungen bzw. Plännen der Partei. Inkludiere Zahlen zu den Forderungen und Plännen, so vorhanden:\n",
    "            \n",
    "```\n",
    "{page_text}\n",
    "```\n",
    "        \"\"\", 4096)\n",
    "        summaries.append(f'========== PAGE {page_number}\\n{summary}\\n')\n",
    "\n",
    "    summary = ''.join(summaries)    \n",
    "    write_file(outputfile, summary)\n",
    "\n",
    "def vectorize_file(filename, label):\n",
    "    lines = read_file(filename).splitlines()    \n",
    "    batchLines = []    \n",
    "    batchPages = []\n",
    "    current_tokens = 0    \n",
    "    page = 0\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        line_tokens = num_tokens(line)\n",
    "        if line.startswith(\"======\"):\n",
    "            page += 1\n",
    "            continue\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        if current_tokens + line_tokens > 8000:\n",
    "            response = client.embeddings.create(input=batchLines, model=\"text-embedding-3-small\")\n",
    "            save_vectors(response, label, batchLines, batchPages)\n",
    "            batchLines = []\n",
    "            batchPages = []\n",
    "            current_tokens = 0\n",
    "\n",
    "        batchLines.append(line)\n",
    "        batchPages.append(page)\n",
    "        current_tokens += line_tokens\n",
    "\n",
    "    if batchLines:\n",
    "        response = client.embeddings.create(input=batchLines, model=\"text-embedding-3-small\")\n",
    "        save_vectors(response, label, batchLines, batchPages)    \n",
    "\n",
    "def save_vectors(response, label, lines, pages):    \n",
    "    with open(OUTPUT_DIR + \"vectors.tsv\", 'a') as f:\n",
    "        for i, embedding in enumerate(response.data):\n",
    "            f.write('\\t'.join(map(str, embedding.embedding)) + '\\n')\n",
    "    with open(OUTPUT_DIR + \"vectors.meta.tsv\", 'a') as f:\n",
    "        for i, line in enumerate(lines):\n",
    "            f.write(label + \"\\t\" + str(pages[i]) + \"\\t\" + label + line + \"\\n\")\n",
    "\n",
    "def convert():\n",
    "    for file in files:\n",
    "        text = convert_pdf(file)\n",
    "        party = os.path.splitext(os.path.basename(file))[0]\n",
    "        txt_file = OUTPUT_DIR + party + \".txt\"\n",
    "        summary_file = OUTPUT_DIR + party + \"-summary.txt\"\n",
    "        write_file(txt_file, text)\n",
    "        if not os.path.exists(summary_file):\n",
    "            summarize_file(party, txt_file, summary_file)\n",
    "        vectorize_file(summary_file, party)\n",
    "\n",
    "def project():\n",
    "    data = pd.read_csv(OUTPUT_DIR + \"vectors.tsv\", sep='\\t', header=None)\n",
    "    umap_2d = umap.UMAP(n_components=2, n_neighbors=10, n_epochs=500, random_state=42, metric=\"cosine\")\n",
    "    projection_2d = umap_2d.fit_transform(data)    \n",
    "\n",
    "    np.savetxt(OUTPUT_DIR + \"projection-2d.tsv\", projection_2d, delimiter='\\t')    \n",
    "\n",
    "files = [\"data/fpö.pdf\", \"data/grüne.pdf\", \"data/kpö.pdf\", \"data/neos.pdf\", \"data/övp.pdf\", \"data/spö.pdf\"]\n",
    "if CLEAN and os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "if os.path.exists(OUTPUT_DIR + \"vectors.tsv\"):\n",
    "    os.remove(OUTPUT_DIR + \"vectors.tsv\")\n",
    "if os.path.exists(OUTPUT_DIR + \"vectors.meta.tsv\"):\n",
    "    os.remove(OUTPUT_DIR + \"vectors.meta.tsv\")\n",
    "with open(OUTPUT_DIR + \"vectors.meta.tsv\", 'a') as f:\n",
    "    f.write(\"party\\tpage\\tstatement\\n\")\n",
    "convert()\n",
    "\n",
    "project()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
